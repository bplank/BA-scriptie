{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Machine Learning (ML)\n",
    "\n",
    "by [@barbara_plank](https://twitter.com/barbara_plank)\n",
    "\n",
    "[with parts inspired by many, amongst which: Anders Johannsen, Malvina, sklearn tutorial.. thanks!]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning = learning from data\n",
    "\n",
    "learning what? \n",
    "\n",
    "to make **predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* is today a good day to get an ice cream?\n",
    "* what is the sentiment of this tweet?\n",
    "* how is the weather in 24h from now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do you do in front of a zebra crossing?\n",
    "\n",
    "<img src=\"pics/zebracrossing.jpg\">\n",
    "[Example inspired by traffic light by M.Nissim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zebra crossing\n",
    "\n",
    "**STOP** or **GO**\n",
    "\n",
    "How can we teach someone this behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* create ad hoc **rules** (as exhaustive as possible)\n",
    "* collect a set of real **examples** of what people do at a zebra crossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "collect **examples** (cases) of zebra crossings and people's behavior (stop or go)\n",
    "\n",
    "* zebra crossing $\\rightarrow$ **features** (characteristics)\n",
    "* result $\\rightarrow$ **label** (category: stop, go)\n",
    "\n",
    "with these examples we can use machine learning to **induce** a classifier (= **build a predictor**) that **generalizes** from the observed exampels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why can't we just build a predictor by coding it up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* can't be exhaustive enough\n",
    "* often we don't know how\n",
    "* trade-off between cost of obtaining **data** versus **knowledge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning versus traditional programming\n",
    "\n",
    "<img src=\"pics/prog-vs-ml.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we know that our model generalized?\n",
    "\n",
    "We want to build a classifier that generalizes, i.e., that works *beyond* the training data.\n",
    "\n",
    "It generalizes reasonably well if it can predict well on new **unseen** test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning is ubiquitous\n",
    "\n",
    "* recommended books in online book stores\n",
    "* your spam classifier\n",
    "* automatic machine translation\n",
    "* NetFlix movie recommendation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML is the future, and you know it\n",
    "*Name one thing that computers cannot do today but might be able to accomplish in five years.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- \"Make interesting conversational partners\"\n",
    "- \"Flawless object recognition (when objects are shown from an unfamiliar angle)\"\n",
    "- \"Cook food via robots?\"\n",
    "- \"Having AI similar to humans ... Strong AI.\"\n",
    "- \"Summarize the plot of a movie by visual analysis.\"\n",
    "\n",
    "[examples from AJ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "In classification we assign a *discrete* label to an object.\n",
    "\n",
    "<img src=\"pics/running.jpg\">\n",
    "\n",
    "\n",
    "For instance, **what kind of food is passing on the running belt**?\n",
    "\n",
    "In programming terms, a classifier is an algorithm for deciding which category the object belongs to.\n",
    "In math terms, a classifier is a function that maps the object to a set of discrete categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Function notation\n",
    "\n",
    "$$f: \\mathbb{R} \\mapsto \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def triple(a_number):\n",
    "    return 3 * a_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f: \\mathbb{R} \\mapsto \\{-1, 1\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def is_expensive_house(house_price):\n",
    "    if house_price > 1000000:\n",
    "        return 1\n",
    "    else: \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifier as a function\n",
    "\n",
    "Formally, we can think of a classifier as a mathmatical function $h$, mapping from the input to one of $k$ output categories. Often the input is a vector of real numbers.\n",
    "\n",
    "$$h: \\mathbb{R}^d \\mapsto \\{1, 2, \\ldots, k\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In some cases our instances can be represented by a binary vector\n",
    "\n",
    "$$h: \\mathbf{2}^d \\mapsto \\{1, 2, \\ldots, k\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# `instance` is a set of properties\n",
    "def classify_animal(instance):\n",
    "    if 'extinct' in instance and 'feathered' in instance:\n",
    "        return 'dinosaur'\n",
    "    elif 'feathered' in instance:\n",
    "        return 'bird'\n",
    "    else:\n",
    "        return 'mammal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Machine learning algorithms are a special kind of algorithms that take data as input and return a new algorithm as output. E.g. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f: \\mathcal{D} \\mapsto \\left(\\mathbb{R}^d \\mapsto \\{1, 2, \\ldots, k\\}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Machine learning classification algorithms differ with respect to \n",
    "\n",
    "- What kind of input they can learn from (labeled, partly labeled, unlabeled).\n",
    "- How the hypothesis function $h$ is represented.\n",
    "- How well the hypothesis $h$ generalizes to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we need\n",
    "\n",
    "1. Data\n",
    "  * what your data looks, the input $X$ and output (labels) $Y$ \n",
    "2. Features\n",
    "  * how to represent your data (the actual features): how to decompose $X$ into its parts by $\\phi$\n",
    "3. Model/Algorithm\n",
    "  * the machine learning algorithm used \n",
    "4. Evaluation\n",
    "  * how to measure how good your model is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To visualize the whole:\n",
    "\n",
    "<img src=\"pics/learning.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification vs Regression\n",
    "\n",
    "\n",
    "The goal of machine learning is to find a function $f$ that, given some input $x$, produces predictions for that input, $y$.\n",
    "\n",
    "In **supervised machine learning** the y’s are given, and are called the labels. They can be categorial, like ”sports”, ”news”, etc. or numerical, e.g. 7, 8,10. If the labels are categorical we speak of classification, in case of numerical labels the task is regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a classifier on the IRIS dataset\n",
    "\n",
    "In this section, we will train a classifier on the [IRIS data set](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html). \n",
    "\n",
    "<img src=\"pics/iris.png\" width=300>\n",
    "This data set about plants is included in sklearn and already ready to use, meaning that features are already extracted for the data instances x, and each training instance has an associated class label y. In the next section we will see how to extract features and use them in sklearn.\n",
    "The iris data set consists of 150 training instances with 3 classes (setosa,versicolor,virginica). Technically, it is stored as a python dict, thus we can see dict.keys() to inspect what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The iris data set consists of 150 training instances with 3 classes (setosa,versicolor,virginica). Tech- nically, it is stored as a python dict, thus we can see dict.keys() to inspect what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'DESCR', 'target', 'feature_names', 'target_names'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], \n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the possible Y's (labels/categories)\n",
    "iris['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking at the labels of the IRIS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "## in sklearn labels/categories are actually encoded as numbers!\n",
    "print(np.unique(iris_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do the examples/data instances/cases look like?\n",
    "\n",
    "Each training instance consists of 4 attributes (thus is 4-dimensional, or a vector with 4 dimensions), in this case numerical measurements. We can get a description using feature_names. For instance, lets look at the first data instance $x_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.1,  3.5,  1.4,  0.2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://www.wpclipart.com/plants/diagrams/plant_parts/petal_sepal_label.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The class labels y for the instances are actually stored in iris[’target’] as integers (indices corresponding to the respective target_names entry). Thus, the first instances is of type setosa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we are ready to train a classifier on 80% of the data, and evaluate on the remaining 20%. We will train both a k-nearest neighbor classifier as well as logistic regression model, and evaluate both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#inst train: 120\n",
      "#inst test: 30\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Pred: [1 1 0 2 1 1 2 1 0 2 2 2 2 1 0 0 1 0 0 1 2 1 0 0 2 2 1 1 2 1]\n",
      "Gold: [1 1 0 1 1 1 1 1 0 2 2 2 2 1 0 0 1 0 0 1 2 1 0 0 2 2 1 1 1 1]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00         8\n",
      " versicolor       1.00      0.80      0.89        15\n",
      "  virginica       0.70      1.00      0.82         7\n",
      "\n",
      "avg / total       0.93      0.90      0.90        30\n",
      "\n",
      "[[ 8  0  0]\n",
      " [ 0 12  3]\n",
      " [ 0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# 3 classes, 150 instances: X=iris[’data’]  y=iris[’target’]\n",
    "iris = datasets.load_iris()\n",
    "# create random permutation with seed (uncomment to get fixed set)\n",
    "#np.random.seed(1253)\n",
    "indices = np.random.permutation(len(iris['data']))\n",
    "# split in 80% train, 20% test\n",
    "len_test = int(len(iris['data'])*0.2)\n",
    "# train part (all except test part)\n",
    "X_train = iris['data'][indices[:-len_test]]\n",
    "y_train = iris['target'][indices[:-len_test]]\n",
    "# test part\n",
    "X_test = iris['data'][indices[-len_test:]]\n",
    "y_test = iris['target'][indices[-len_test:]]\n",
    "# output statistics\n",
    "print(\"#inst train: %s\" % (len(X_train)))\n",
    "print(\"#inst test: %s\" % (len(X_test)))\n",
    "# learn knn classifier\n",
    "knn = LogisticRegression()\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn)\n",
    "y_pred= knn.predict(X_test)\n",
    "print(\"Pred:\", y_pred)\n",
    "print(\"Gold:\", y_test)\n",
    "# get accuracies\n",
    "print(classification_report(y_test, y_pred, target_names=iris['target_names']))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAREAAAEACAYAAACUHkKwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADlVJREFUeJzt3W+MXXWdx/H3p1YeWBvWf4xaoG78U6LZhq2hQtikk7gq\nRbQ8MOu/ROWBSwxEEx8sriGhmH0gz1xWEpaEFTAxkpgoKOqCMdWwG5EUKqxb2pJFxRaqG8QMYlZo\nv/tgLmRsZzrTfs/MvYX3K7mZc+/53vP79tfOZ84599c2VYUknahV425A0snNEJHUYohIajFEJLUY\nIpJaDBFJLas7b07yCuBWYD3wC+Dvqur389T9Avg9cBh4pqo2d8aVNDm6ZyKfA35QVRuAHwL/uEDd\nYWC6qv7aAJFeWLohsg24ebR9M3DxAnUZYCxJE6j7jX1aVR0EqKrHgdMWqCvgriT3Jvlkc0xJE2TR\neyJJ7gKm5r7EbChcOU/5Qmvoz6+qx5K8htkw2V1Vdx93t5ImzqIhUlXvWmhfkoNJpqrqYJLXAr9Z\n4BiPjb7+Nsk3gc3AvCGSxL/MI41JVeV439P6dAa4HfgEcA3wceC2IwuSvAxYVVVPJVkDvBu4+lgH\n3bRpU7OtYR04cIDXv/71427jz7zvfe9j+/bt427jz2zfvt2elmgS+0qOOz+A/j2Ra4B3JdkDvBP4\n4qiZ1yX5zqhmCrg7yf3AT4BvV9WdzXElTYjWmUhVPQH87TyvPwZcNNp+BDi7M46kyeXHrkuwdu3a\ncbdwlOnp6XG3cBR7WrpJ7etEZNL+UaIkNWn3RCbRzp07x92CXmCSnNCNVc9EJLUYIpJaDBFJLYaI\npBZDRFKLISKpxRCR1GKISGoxRCS1GCKSWgwRSS2GiKQWQ0RSiyEiqcUQkdRiiEhqMUQktRgikloM\nEUkthoikFkNEUoshIqnFEJHUYohIajFEJLUYIpJaBgmRJBckeSjJ3iRXLFBzbZJ9SXYl8T/4ll4g\n2iGSZBXwZeA9wNuADyc564iarcAbq+rNwKXA9d1xJU2GIc5ENgP7quqXVfUM8HVg2xE124BbAKrq\nHuDUJFMDjC1pzIYIkXXAo3Oe/3r02rFq9s9TI+kk5I1VSS2rBzjGfuDMOc9PH712ZM0Zi9Q878CB\nA89vr127lrVr1/a7lPRnduzYwY4dO9rHSVX1DpC8BNgDvBN4DPgp8OGq2j2n5kLgsqp6b5JzgS9V\n1bkLHK82bdrU6unFYOfOneNuQS8wSaiqHO/72mciVXUoyeXAncxeHt1YVbuTXDq7u26oqu8muTDJ\nw8AfgEu640qaDO0zkaF5JrI0noloaCd6JuKNVUkthoikFkNEUoshIqnFEJHUYohIajFEJLUYIpJa\nDBFJLYaIpBZDRFKLISKpxRCR1GKISGoxRCS1GCKSWgwRSS2GiKQWQ0RSiyEiqcUQkdRiiEhqMUQk\ntRgikloMEUkthoikFkNEUoshIqnFEJHUMkiIJLkgyUNJ9ia5Yp79W5I8meS+0ePKIcaVNH6ruwdI\nsgr4MvBO4ABwb5LbquqhI0p/XFXv744nabIMcSayGdhXVb+sqmeArwPb5qnLAGNJmjBDhMg64NE5\nz389eu1I5yXZleSOJG8dYFxJE6B9ObNEO4Ezq+rpJFuBbwFvWaj4oosuen57y5YtTE9PL3uDJ5tV\nq7wnvhTXX3/9uFuYWHv27GHv3r0ArFmz5oSPk6pqNZLkXGB7VV0wev45oKrqmmO85xHg7VX1xDz7\n6tChQ62eXgxWr16p/D+5GSJLMzU1xcUXX0xVHfdthyF+nN0LvCnJ+iSnAB8Cbp9bkGRqzvZmZsPr\nqACRdPJp/zirqkNJLgfuZDaUbqyq3Ukund1dNwAfSPIp4Bngj8AHu+NKmgyDnBNX1feBDUe89q9z\ntq8DrhtiLEmTxbtzkloMEUkthoikFkNEUoshIqnFEJHUYohIajFEJLUYIpJaDBFJLYaIpBZDRFKL\nISKpxRCR1GKISGoxRCS1GCKSWgwRSS2GiKQWQ0RSiyEiqcUQkdRiiEhqMUQktRgikloMEUkthoik\nFkNEUssgIZLkxiQHkzxwjJprk+xLsivJ2UOMK2n8hjoT+QrwnoV2JtkKvLGq3gxcClw/0LiSxmyQ\nEKmqu4HfHaNkG3DLqPYe4NQkU0OMLWm8VuqeyDrg0TnP949ek3SSWz3uBuZz9dVXP7+9ZcsWpqen\nx9eM9AK1Z88e9u7dC8CaNWtO+DgrFSL7gTPmPD999Nq8rrrqqmVvSHqx27BhAxs2bABgamqKW2+9\n9YSOM+TlTEaP+dwOfAwgybnAk1V1cMCxJY3JIGciSb4GTAOvSvIr4CrgFKCq6oaq+m6SC5M8DPwB\nuGSIcSWN3yAhUlUfWULN5UOMJWmyuGJVUoshIqnFEJHUYohIajFEJLUYIpJaDBFJLYaIpBZDRFKL\nISKpxRCR1GKISGoxRCS1GCKSWgwRSS2GiKQWQ0RSiyEiqcUQkdRiiEhqMUQktRgikloMEUkthoik\nFkNEUoshIqnFEJHUYohIahkkRJLcmORgkgcW2L8lyZNJ7hs9rhxiXEnjt3qg43wF+BfglmPU/Liq\n3j/QeJImxCBnIlV1N/C7RcoyxFiSJstK3hM5L8muJHckeesKjitpGQ11ObOYncCZVfV0kq3At4C3\nLFT8hS984fnt6elppqenl73Bk82zzz477hZOCuecc864W5hYMzMzzMzMADA1NXXCx0lVDdJQkvXA\nt6tq4xJqHwHeXlVPzLOvhurphezw4cPjbuGkYIgszcaNG7npppuoquO+7TDk5UxY4L5Hkqk525uZ\nDa+jAkTSyWeQy5kkXwOmgVcl+RVwFXAKUFV1A/CBJJ8CngH+CHxwiHEljd8gIVJVH1lk/3XAdUOM\nJWmyuGJVUoshIqnFEJHUYohIajFEJLUYIpJaDBFJLYaIpBZDRFKLISKpxRCR1GKISGoxRCS1GCKS\nWgwRSS2GiKQWQ0RSiyEiqcUQkdRiiEhqMUQktRgikloMEUkthoikFkNEUoshIqnFEJHUYohIammH\nSJLTk/wwyc+TPJjk0wvUXZtkX5JdSc7ujitpMqwe4BjPAp+tql1JXg7sTHJnVT30XEGSrcAbq+rN\nSd4BXA+cO8DYksasfSZSVY9X1a7R9lPAbmDdEWXbgFtGNfcApyaZ6o4tafwGvSeS5A3A2cA9R+xa\nBzw65/l+jg4aSSehwUJkdCnzDeAzozMSSS8CQ9wTIclqZgPkq1V12zwl+4Ez5jw/ffTavLZv3/78\n9vT0NNPT00O0KWmOmZkZZmZmADh06NAJHydV1W4myS3A/1bVZxfYfyFwWVW9N8m5wJeqat4bq0lq\niJ5e6A4fPjzuFk4K55xzzrhbOCls3LiRm266iarK8b63fSaS5Hzgo8CDSe4HCvg8sB6oqrqhqr6b\n5MIkDwN/AC7pjitpMrRDpKr+A3jJEuou744lafK4YlVSiyEiqcUQkdRiiEhqMUQktRgikloMEUkt\nhoikFkNEUoshIqnFEJHUYohIajFEJLUYIpJaDBFJLYaIpBZDRFKLISKpxRCR1GKISGoxRCS1GCKS\nWgwRSS2GiKQWQ0RSiyEiqcUQkdRiiEhqaYdIktOT/DDJz5M8mOTT89RsSfJkkvtGjyu740qaDEOc\niTwLfLaq3gacB1yW5Kx56n5cVZtGj38aYNwVs2PHjnG3cBR7WpqZmZlxtzCvSe3rRLRDpKoer6pd\no+2ngN3AunlK0x1rXCbxm+NHP/rRuFs4yiT2NKnfrJPa14kY9J5IkjcAZwP3zLP7vCS7ktyR5K1D\njitpfFYPdaAkLwe+AXxmdEYy107gzKp6OslW4FvAW4YaW1rIq1/9as46a76r6/H605/+NFF9rV+/\n/oTfm6pqN5BkNfAd4HtV9c9LqH8EeHtVPTHPvn5Dkk5IVR33bYehzkT+DfjvhQIkyVRVHRxtb2Y2\nvI4KEDixX4Sk8WmHSJLzgY8CDya5Hyjg88B6oKrqBuADST4FPAP8Efhgd1xJk2GQyxlJL15jXbGa\n5BVJ7kyyJ8m/Jzl1gbpfJPlZkvuT/HSZerkgyUNJ9ia5YoGaa5PsG33KdPZy9HG8fa30Qr4kNyY5\nmOSBY9Ss6Dwt1tM4FjsuZRHmqG6l52r4xaFVNbYHcA3wD6PtK4AvLlD3P8ArlrGPVcDDzF6CvRTY\nBZx1RM1W4I7R9juAn6zA/Cylry3A7Sv4e/Y3zH6M/8AC+8cxT4v1tKJzNBrztcDZo+2XA3sm5M/U\nUvo6rvka99+d2QbcPNq+Gbh4gbqwvGdNm4F9VfXLqnoG+Pqot7m2AbcAVNU9wKlJppaxp6X2BSu4\nkK+q7gZ+d4ySFZ+nJfQEK7zYsZa2CHMcczX44tBxh8hpNfrUpqoeB05boK6Au5Lcm+STy9DHOuDR\nOc9/zdETe2TN/nlqxtEXTNZCvnHM01KMbY6OsQhzrHM11OLQwRabLSTJXcDcdA2zoTDfddZCd3nP\nr6rHkryG2TDZPfrpIxfyLcXY5miRRZhjM+Ti0GU/E6mqd1XVxjmPvxp9vR04+NzpW5LXAr9Z4BiP\njb7+Fvgms6f5Q9oPnDnn+emj146sOWORmqEt2ldVPVVVT4+2vwe8NMkrl7mvYxnHPB3TuOZotAjz\nG8BXq+q2eUrGMleL9XW88zXuy5nbgU+Mtj8OHPULSvKyUWqSZA3wbuC/Bu7jXuBNSdYnOQX40Ki3\nI3v92KiPc4Enn7sUW0aL9jX3GnqxhXwDCgtfM49jno7Z05jmCBZZhMn45mrRxaFzthefr5W8Yz3P\nneJXAj9g9g7xncBfjF5/HfCd0fZfMvupxP3Ag8DnlqmXC0Z97HtuDOBS4O/n1HyZ2U9LfgZsWqE5\nOmZfwGXMhur9wH8C71jmfr4GHAD+D/gVcMm452mxnlZ6jkZjng8cmvNn977R7+W452rRvo53vlxs\nJqll3Jczkk5yhoikFkNEUoshIqnFEJHUYohIajFEJLUYIpJa/h9dnzOwxEL5eAAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f00bdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "plt.imshow(conf, cmap='binary', interpolation='None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0   1  2  All\n",
      "True                    \n",
      "0          8   0  0    8\n",
      "1          0  10  0   10\n",
      "2          0   4  8   12\n",
      "All        8  14  8   30\n"
     ]
    }
   ],
   "source": [
    "## an alternative\n",
    "import pandas as pd\n",
    "def crosstab(pred, gold):\n",
    "    y_true = pd.Series(gold)\n",
    "    y_pred = pd.Series(pred)\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    \n",
    "crosstab(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "* Use the K-nearest neighbor classifier for the Iris dataset. Compare its performance to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features, and feature templates\n",
    "\n",
    "In NLP, we typically have a *lot* of features (not just the 4 we saw in the IRIS example). Typically, we work with entire *vocabularies*. When we speak of a *feature* in NLP we typically mean a *feature template* (which gets instantiated to a set of values). For example, when we say we use unigrams (single words) as feature, this is actually a feature template that gets instantiated. Or when we say we use POS tags. Internally, these every value of a feature template is mapped to a unique feature (feature id), and if this feature is 'on' (or active), we'll see a 1 in the place of the vector that represents this features (e.g. POS tag is \"DET\").\n",
    "\n",
    "You can imagine that every instance is represented as a long vector (a high-dimensional vector; each dimension is a features). If you have 10000 features, you can imagine a vector of 10000 length, or 1000 dimensions. For every feature that is active for a given example, you turn the vector dimension 'on' (save it as a 1). \n",
    "\n",
    "Now in practice, we won't work with superlong vectors. They are becoming easily too big to process. Why?\n",
    "\n",
    "Read 'bag of words' of the tutorial: [http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) \n",
    "\n",
    "* features, feature templates\n",
    "* sparse representations\n",
    "\n",
    "Sklearn is handling the data in sparse format for you. You don't need to worry about it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Sentiment Classification\n",
    "\n",
    "Go to the folder 'exercise'. Retrieve the code. Go through the code with your neighbor. Fill\n",
    "out the blanks. Make sure you understand all parts of the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Machine Learning\n",
    "\n",
    "For you there to read, a more formal view.\n",
    "\n",
    "### Input and output\n",
    " \n",
    "The goal of supervised machine learning is to find a function $h$ that maps from some percept or input $x$ to a label $y$. What $x$ and $y$ are depends on the task. Many banks, for instance, use a learned function to decide whether to give credit to a customer or not. Here, $x$ is the credit application and $y$ is the outcome: approved or declined. In NLP, $x$ could be a tweet and $y$ could be its sentiment, or $x$ could be a sentence and $y$ is syntactic parse tree; and so forth. Let $x \\in \\mathcal{X}$ (input space) and $y \\in \\mathcal{Y}$ (label space).\n",
    "\n",
    "NLP applications almost always have **discrete** output spaces. In these lectures $y$ will either be an integer (for classification) or a vector of integers (for structured prediction). \n",
    "\n",
    "### Target and hypothesis function\n",
    "\n",
    "We’ll make the assumption that there exists an **unknown target function** which is solving the problem we’re interested in:\n",
    "\n",
    "$$f: \\mathcal{X} \\mapsto \\mathcal{Y}$$\n",
    "\n",
    "This, of course, is a bit of a fiction. It doesn’t really exist anywhere, but it’s a useful fiction because it allows us to describe the goal, which is to learn a **hypothesis function** $h$ that is as close as possible to the target function. Naturally, the hypothesis function performs the same mapping as the unknown target function:\n",
    "\n",
    "$$h: \\mathcal{X} \\mapsto \\mathcal{Y}$$\n",
    "\n",
    "### Dataset \n",
    "\n",
    "It gets worse before it gets better. Not only is our target function unknown, we also don’t know the true distribution of our inputs $P(x)$. We don’t know which tweets will be written or the kinds of backgrounds people who apply for credit will have.   \n",
    "\n",
    "Supervised learning rests on the idea that we can get a limited number of examples (i.e. **a sample**) \n",
    "\n",
    "$$x_1, \\ldots, x_n \\sim P(x)$$\n",
    "\n",
    "from the unknown input distribution $P(x)$, and that we (somehow) can evaluate on the unknown target function $f$ on these examples. \n",
    "\n",
    "Putting this together yields the concept of a **training set**:\n",
    "\n",
    "$$\\mathcal{D}_t = \\{(x_1, f(x_1) ), \\ldots (x_n, f(x_n)) \\}$$\n",
    "\n",
    "How do we gain access to the unknown target function? The bank might look at past credit applications together with the decisions. In NLP we often ask *people* to annotate.\n",
    "\n",
    "#### Unsupervised and semi-supervised learning\n",
    "\n",
    "It’s easy to imagine a situation where we could arrange to get a large sample of data from $P(x)$ without labels being included in the deal. The setting in which there are no labels at all is called **unsupervised learning**. When unlabeled data is available in addition to a labeled dataset this is **semi-supervised learning**. \n",
    "\n",
    "### Feature representation\n",
    "\n",
    "We’ll never have to read the same Twitter message twice, hopefully. By the time a failed credit application is resubmitted, the customer’s circumstances are likely different, and so the  application isn’t the same anymore. “You cannot submit a credit application twice,” as Heraclitus might have said. \n",
    "\n",
    "This poses a problem in that we wish to learn from the past, but whatever happened in the past it will not happen *exactly* like that again. Instead something *similar* might happen. So we need a way to break up our observations (the $x$es) to make them comparable even if the don’t match exactly. \n",
    "\n",
    "Luckily, our observations are typically not unique snowflakes, but can decomposed into **features** in some **feature space** $\\mathcal{F}$. Even though the learner might not have seen the new example exactly, it might have seen similar examples (or parts of the current example), and thus still be able to make a prediction.\n",
    "\n",
    "Specifically, each input example is transformed into a suitable **input representation** for the learning algorithm by a **feature function** $\\phi(x)$. The feature function $\\phi(\\cdot)$ maps examples from the input space to the feature space:\n",
    "\n",
    "$$\\phi: \\mathcal{X} \\rightarrow \\mathcal{F}$$\n",
    "\n",
    "Typically, the $\\phi(x)$ is a real-valued vector of some fixed dimension $d$, i.e. \n",
    "\n",
    "$$\\mathcal{F} = \\mathbb{R}^d$$\n",
    "\n",
    "Note that the $\\phi$ feature function is deterministic and not a part of the learner. Traditionally, a large body of work in NLP focused on finding better ways to map from input to feature representations for specific tasks by hand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "* [sklearn: Working with text data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "* Malvina Nissim and Johannes Bjerva. Learning from data, [ESSLLI 2016 lecture 1](http://esslli2016.unibz.it/wp-content/uploads/2015/10/lecture1.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
