{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding good features\n",
    "\n",
    "In Machine Learning, it is important to find good features that help to predict the target variables. Finding good features will help us to build better classifiers. But once we have trained a good model, we might also want to know which features were the most useful in classifing the instances. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: stability selection -  Training a classifier on the IRIS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will again train a classifier on the [IRIS data set](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html) and use stability selection to find the strongest features.\n",
    "\n",
    "<img src=\"pics/iris.png\" width=300>\n",
    "This data set about plants is included in sklearn and already ready to use, meaning that features are already extracted for the data instances x, and each training instance has an associated class label y. \n",
    "The iris data set consists of 150 training instances with 3 classes (setosa,versicolor,virginica). Lets train a classifier and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#inst train: 120\n",
      "#inst test: 30\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Pred: [0 2 1 2 2 2 2 1 0 0 0 1 1 0 0 0 2 1 2 2 0 0 1 1 0 0 0 1 0 2]\n",
      "Gold: [0 2 1 2 2 2 2 1 0 0 0 1 1 0 0 0 2 1 2 2 0 0 1 1 0 0 0 1 0 2]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        13\n",
      " versicolor       1.00      1.00      1.00         8\n",
      "  virginica       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n",
      "[[13  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression, RandomizedLogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 3 classes, 150 instances: X=iris[’data’]  y=iris[’target’]\n",
    "iris = datasets.load_iris()\n",
    "indices = np.random.permutation(len(iris['data']))\n",
    "# split in 80% train, 20% test\n",
    "len_test = int(len(iris['data'])*0.2)\n",
    "# train part (all except test part)\n",
    "X_train = iris['data'][indices[:-len_test]]\n",
    "y_train = iris['target'][indices[:-len_test]]\n",
    "# test part\n",
    "X_test = iris['data'][indices[-len_test:]]\n",
    "y_test = iris['target'][indices[-len_test:]]\n",
    "# output statistics\n",
    "print(\"#inst train: %s\" % (len(X_train)))\n",
    "print(\"#inst test: %s\" % (len(X_test)))\n",
    "# learn knn classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)\n",
    "y_pred= clf.predict(X_test)\n",
    "print(\"Pred:\", y_pred)\n",
    "print(\"Gold:\", y_test)\n",
    "# get accuracies\n",
    "print(classification_report(y_test, y_pred, target_names=iris['target_names']))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is doing a pretty good job. But which features were the most predictive ones? To answer this question, we will look at stability selection. \n",
    "\n",
    "### Stability selection\n",
    "Stability selection [(Meinshausen & Bühlmann, 2009)](http://stat.ethz.ch/~nicolai/stability.pdf) is a relatively novel model for feature selection. It is based on subsampling in combination with a learning algorithm. The main idea is to apply a feature selection algorithm on different subset of data using different subsets of features. This procedure is then iterated. At the end, strong features will appear in many subsamples, since they will often be selected. The output of stability selection is a probability score per feature, which indicates how likely it is that this feature is selected.\n",
    "\n",
    "Stability selection is implemented in `sklearn` in `RandomizedLogisticRegression`. It trains several LogisticRegression models on subsets of the data, using a feature selection algorithm (L1 regularisation). The algorithm records how often the feature has been selected, and hence the final score gives an indication of importance of a feature for a given task.\n",
    "\n",
    "From the `sklearn` documentation:\n",
    "\"Randomized Logistic Regression works by subsampling the training data and fitting a L1-penalized LogisticRegression model where the penalty of a random subset of coefficients has been scaled. By performing this double randomization several times, the method assigns high scores to features that are repeatedly selected across randomizations. This is known as stability selection. In short, features selected more often are considered good features.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "0.685 sepal width (cm)\n",
      "0.64 petal width (cm)\n",
      "0.635 petal length (cm)\n",
      "0.145 sepal length (cm)\n"
     ]
    }
   ],
   "source": [
    "# lets use stabiliy selection\n",
    "randomLogReg = RandomizedLogisticRegression()\n",
    "randomLogReg.fit(X_train, y_train)\n",
    "\n",
    "names = iris['feature_names']\n",
    "print(\"Features sorted by their score:\")\n",
    "for score, name in sorted(zip(map(lambda x: round(x, 4), randomLogReg.scores_), names), reverse=True):\n",
    "    print(score, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We now see that petal length is amongst the most predictive features. However, we do not see how predictive it was for each class. For this, we inspect the coefficient per class, as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_num 0\n",
      "\t-2.1721\tpetal length (cm)\t\t1.3932\tsepal width (cm)\n",
      "\t-1.0098\tpetal width (cm)\t\t0.4121\tsepal length (cm)\n",
      "class_num 1\n",
      "\t-1.5053\tsepal width (cm)\t\t0.5530\tpetal length (cm)\n",
      "\t-1.0418\tpetal width (cm)\t\t0.3650\tsepal length (cm)\n",
      "class_num 2\n",
      "\t-1.5483\tsepal length (cm)\t\t2.2953\tpetal width (cm)\n",
      "\t-1.3097\tsepal width (cm)\t\t2.1763\tpetal length (cm)\n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "feature_names = iris['feature_names']\n",
    "for class_num in range(0,len(clf.coef_)):\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[class_num], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"class_num\",class_num)\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "The output 'class_num 0' probably doesn't tell you much. Add the name of the current target label to the function above (hint: 'target_names'). Now inspect the most predictive features and look at the various types of iris, do the most predictive features intuitively make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Extend the sentiment classification example, find the most predictive feature per class.\n",
    "\n",
    "Hint: a method you can use is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i in range(0,len(clf.coef_)):\n",
    "        coefs_with_fns = sorted(zip(clf.coef_[i], feature_names))\n",
    "        top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "        print(\"i\",i)\n",
    "        for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "            print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "There is much more to be said about feature selection than what we cover here. Below are some pointers.\n",
    "\n",
    "* http://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "* http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "* http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: in sklearn 0.17 I got a deprecated warning for RandomizedLogisticRegression, upgrading solved it.\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], \n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
